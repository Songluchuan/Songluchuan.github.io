<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Luchuan Song</title>

  <meta name="author" content="Luchuan Song">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Luchuan Song</name>
                  </p>
                  <div class="content has-text-justified">
                  <p style="text-align: justify;">
                     I am a third-year Ph.D. student in the Computer Science Department at the University of Rochester (UofR). My advisor is <a
                     href="https://www.cs.rochester.edu/~cxu22/">Prof. Chenliang Xu</a>. Before that, I received my master and bachelor degree from University of Science and Technology of China (USTC) under the supervision of <a
                     href="https://dsxt.ustc.edu.cn/zj_ywjs.asp?zzid=728">Prof. Nenghai Yu</a> and <a
                     href="https://scholar.google.com/citations?user=kReWULQAAAAJ&hl=zh-CN">Prof. Bin Liu</a>. 
                  
                    </br>
                    </br>
                     I am focusing on human related topic (e.g. face animation and stylization, 3D face reconstruction, deepfake detection e.t.c.). And I also working on the egocentric video understanding, object detection/segmentation and optical character recognition (OCR). Insead of research, I often mountaineering and have summitted the Muztagh Ata and Chola mountains.
                  </p>
                  </div>
                  <p style="text-align:center">
                    <a href="mailto:slc0826@mail.ustc.edu.cn">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=UVsyxOoAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/Songluchuan/">Github</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/lsong11/">Linkedin</a>
                  </p>
                </td>
                <td style="padding: 10% 2% 10% 2%;width:70%;max-width:70%">
                  <img src="images/lsong.jpg" style="float:center" width="78%" alt="photo">
                  <figcaption style="font-size: smaller; color: #A9A9A9;">@On the way to Kawagarbo, Dec. 2022  </figcaption>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <!-- <td style="padding:20px;width:100%;vertical-align:middle"> -->
              <heading>News</heading>
            </td>
          </tr>
        </tbody></table>

        
        <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
<!--             <tr>
              <td valign="top" align="center" style="width: 15%;"><strong>[10/2024]</strong></td>
              <td style="width: 75%;">Part of my works have been posted by <a href="https://www.rochester.edu/newscenter/video-deepfakes-ai-meaning-definition-technology-623572/">UR News
              </a>.
              </td>
            </tr> -->
            <tr>
              <td valign="top" align="center" style="width: 15%;"><strong>[05/2025]</strong></td>
              <td style="width: 75%;">Our workshop on <a href="https://deepfake-workshop-ijcai2025.github.io/main/organizers.html">DeepFake detection</a> (with Ant-Group) will be presented on @ IJCAI 2025!
              </td>
            </tr>
            <tr>
              <td valign="top" align="center" style="width: 15%;"><strong>[04/2025]</strong></td>
              <td style="width: 75%;">Two paper are accepted by @ Siggraph 2025 and @ CVPR 2025 (Highlight) !
              </td>
            </tr>
            <tr>
              <td valign="top" align="center" style="width: 15%;"><strong>[11/2024]</strong></td>
              <td style="width: 75%;">Some of my works about face synthesis are posted by <a href="https://www.rochester.edu/newscenter/video-deepfakes-ai-meaning-definition-technology-623572/">University of Rochester News
              </a>.
              </td>
            </tr>
            <tr>
              <td valign="top" align="center" style="width: 15%;"><strong>[11/2024]</strong></td>
              <td style="width: 75%;">A new paper "Gaussian Head Avatar via StyleGAN" is accepted by @ 3DV 2025 !
              </td>
            </tr>
            <tr>
              <td valign="top" align="center" style="width: 15%;"><strong>[07/2024]</strong></td>
              <td style="width: 75%;">Invited tutorial talk on <a href="https://github.com/yzyouzhang/Awesome-Multimedia-Deepfake-Detection">Multimedia Deepfake Detection
              </a> @ ICME 2024.
              </td>
            </tr>
            <tr>
              <td valign="top" align="center" style="width: 15%;"><strong>[05/2024]</strong></td>
              <td style="width: 75%;">I will be joining <a href="https://research.adobe.com/">Adobe Research</a> for internship, work with <a
                href="https://research.adobe.com/person/yang-zhou/">Dr. Yang Zhou</a>.
              </td>
            </tr>
          </tbody>
      </table>



    </br>
  </br>
</br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <!-- <td style="padding:20px;width:100%;vertical-align:middle"> -->
            <heading>Research</heading>
          </td>
        </tr>
      </tbody></table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/streamme.gif' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://songluchuan.github.io/StreamME/">
                    <papertitle>
                      StreamME: Simplify 3D Gaussian Avatar within Live Stream</papertitle>
                  </a>
                  <br>
                  <strong>Luchuan Song</strong>, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu
                  <br>
                  <em>Siggraph</em>, 2025
                  <br>
                  <a href="https://songluchuan.github.io/StreamME/">project page</a> /
                  <a href="https://camps.aptaracorp.com/ACM_PMS/PMS/ACM/SIGGRAPHCONFERENCEPAPERS25/46/33c12f4a-1b20-11f0-ada9-16bb50361d1f/OUT/siggraphconferencepapers25-46.html">paper</a> /
                  <a href="https://songluchuan.github.io/StreamME/">code</a> 
                  <p>
                   We propose StreamME, a method focuses on fast head reconstruction. It synchronously records/reconstructs head from live video streams without any pre-cached data.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/free-viewpoint-human.gif' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://harlanhong.github.io/publications/fvhuman/index.html">
                    <papertitle>
                      Free-viewpoint Human Animation with Pose-correlated Reference Selection</papertitle>
                  </a>
                  <br>
                  Fa-Ting Hong, Zhan Xu, Haiyang Liu, Qinjie Lin, <strong>Luchuan Song</strong>, Zhixin Shu, Yang Zhou, Duygu Ceylan, Dan Xu
                  <br>
                  <em>CVPR</em>, 2025 (Highlight)
                  <br>
                  <a href="https://harlanhong.github.io/publications/fvhuman/index.html">project page</a> /
                  <a href="https://arxiv.org/pdf/2412.17290">paper</a> /
                  <a href="https://github.com/harlanhong/FVHuman">code</a> 
                  <p>
                   We propose a novel adaptive reference selection diffusion network for free-viewpoint human animation, enabling flexible human framing and versatile camera shot planning without strict reliance on a specific reference image.
                  </p>
                </td>
              </tr>
              
              
              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/sa24.gif' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://songluchuan.github.io/TextToon/">
                    <papertitle>
                      TextToon: Real-Time Text Toonify Head Avatar from Single Video</papertitle>
                  </a>
                  <br>
                  <strong>Luchuan Song</strong>, Lele Chen, Celong Liu, Pinxin Liu, Chenliang Xu
                  <br>
                  <em>Siggraph Asia</em>, 2024
                  <br>
                  <a href="https://songluchuan.github.io/TextToon/">project page</a> /
                  <a href="https://songluchuan.github.io/TextToon/">paper</a> /
                  <a href="https://songluchuan.github.io/TextToon/">code</a> 
                  <p>
                    We present a method to generate a drivable toonified avatar. Given a monocular video and a written instruction about the avatar style, it can generate a toonified avatar that can be animated in real time.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/ECCV2024.gif' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://songluchuan.github.io/Tri2Plane.github.io/">
                    <papertitle>
                      Tri2-plane: Thinking Head Avatar via Feature Pyramid</papertitle>
                  </a>
                  <br>
                  <strong>Luchuan Song</strong>, PinXin Liu, Lele Chen ,Guojun Yin, Chenliang Xu
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <a href="https://songluchuan.github.io/Tri2Plane.github.io/">project page</a> /
                  <a href="https://arxiv.org/abs/2401.09386">paper</a> /
                  <a href="https://github.com/Songluchuan/Tri2plane">code</a> 
                  <p>
                    We attach the multi-combined tri-plane sturcture for monocular photo-realistic volumetric head avatar reconstructions.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/icassp24.gif' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/Songluchuan/AdaSR-TalkingHead">
                    <papertitle>Adaptive Super Resolution for One-Shot Talking Head Generation</papertitle>
                  </a>
                  <br>
                  <strong>Luchuan Song</strong>, Pinxin Liu, Guojun Yin, Chenliang Xu
                  <br>
                  <em>ICASSP</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2403.15944">paper</a> /
                  <a href="https://github.com/Songluchuan/AdaSR-TalkingHead">code</a> /
                  <a href="https://www.youtube.com/watch?v=B_-3F51QmKE">video</a>
                  <p>We apply the mix-resolution images in one-shot talking head training. The resolution could achieve 512px from 256px in previous.</p>
                </td>
              </tr>
              
              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/mm24.gif' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openreview.net/forum?id=mk8p2JKdu0">
                    <papertitle>EAGLE: Egocentric AGgregated Language-video Engine</papertitle>
                  </a>
                  <br>
                  Jing Bi, Yunlong Tang, <strong>Luchuan Song</strong>, Ali Vosoughi, Nguyen Nguyen, Chenliang Xu
                  <br>
                  <em>ACM'MM</em>, 2024
                  <br>
                  <a href="https://openreview.net/pdf?id=mk8p2JKdu0">paper</a> /
                  <a href="https://openreview.net/forum?id=mk8p2JKdu0">code</a>
                  <!-- <a href="https://www.youtube.com/watch?v=B_-3F51QmKE">video</a> -->
                  <p>We introduce the EAGLE (Egocentric AGgregated Language-video Engine) model and dataset for egocentric video understanding tasks.</p>
                </td>
              </tr>
            
              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/nips23.png' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/SegmentationBLWX/sssegmentation">
                    <papertitle>IDRNet: Intervention-driven relation network for semantic segmentation</papertitle>
                  </a>
                  <br>
                  Zhenchao Jin, Xiaowei Hu, Lingting Zhu, <strong>Luchuan Song</strong>, Li Yuan, Lequan Yu
                  <br>
                  <em>NeurIPS</em>, 2023
                  <br>
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/a216c27f2f3160b1785c057fa510fdf1-Paper-Conference.pdf">paper</a> /
                  <a href="https://github.com/SegmentationBLWX/sssegmentation">code</a>
                  <!-- <a href="https://www.youtube.com/watch?v=B_-3F51QmKE">video</a> -->
                  <p>We leverage a deletion diagnostics procedure in image segmentation.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/arxiv23.png' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding">
                    <papertitle>Video understanding with large language models: A surveyn</papertitle>
                  </a>
                  <br>
                  Yunlong Tang, Jing Bi, Siting Xu, <strong>Luchuan Song</strong>, Susan Liang, Teng Wang, ..., Ping Luo, Jiebo Luo, Chenliang Xu
                  <br>
                  <em>Arxiv</em>, 2023
                  <br>
                  <a href="https://arxiv.org/pdf/2312.17432">paper</a> /
                  <a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding">project page</a>
                  <!-- <a href="https://www.youtube.com/watch?v=B_-3F51QmKE">video</a> -->
                  <p>The comprehensive survey covers video understanding techniques powered by large language models (Vid-LLMs).</p>
                </td>
              </tr>


              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/iccv23.gif' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Song_Emotional_Listener_Portrait_Neural_Listener_Head_Generation_with_Emotion_ICCV_2023_paper.html">
                    <papertitle>Emotional Listener Portrait: Neural Listener Head Generation with Emotion</papertitle>
                  </a>
                  <br>
                  <strong>Luchuan Song</strong>, Guojun Yin, Zhenchao Jin, Xiaoyi Dong, Chenliang Xu
                  <br>
                  <em>ICCV</em>, 2023
                  <br>
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Song_Emotional_Listener_Portrait_Neural_Listener_Head_Generation_with_Emotion_ICCV_2023_paper.pdf">paper</a> /
                  <a href="https://www.youtube.com/watch?v=KCzA5dnXf-I">video</a>
                  <p>We propose a method to implement the head movement of the listening head with expression.</p>
                </td>
              </tr>


              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/mm22.png' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547806f">
                    <papertitle>Face Forgery Detection via Symmetric Transformer</papertitle>
                  </a>
                  <br>
                  <strong>Luchuan Song</strong>, Xiaodan Li, Zheng Fang, Zhenchao Jin, YueFeng Chen, Chenliang Xu
                  <br>
                  <em>ACM'MM</em>, 2022
                  <br>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547806">paper</a> /
                  <a href="https://github.com/Songluchuan/MM2022-Deepfake/blob/main/MM_presentation.mp4">video</a> /
                  <a href="https://github.com/Songluchuan/Adaptive-Face-Forgery-Detection-in-Cross-Domain-ECCV-2022">code</a>
                  <p>We apply channel-wise & spatial-wise feature for Deepfake Detection.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/eccv22.png' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136940460.pdf">
                    <papertitle>Adaptive Face Forgery Detection in Cross Domain</papertitle>
                  </a>
                  <br>
                  <strong>Luchuan Song</strong>, Zheng Fang, Xiaodan Li, Xiaoyi Dong, Zhenchao Jin, Yuefeng Chen, Siwei Lyu
                  <br>
                  <em>ECCV</em>, 2022
                  <br>
                  <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136940460.pdf">paper</a> /
                  <a href="https://github.com/Songluchuan/Adaptive-Face-Forgery-Detection-in-Cross-Domain-ECCV-2022">code</a>
                  <!-- <a href="https://www.youtube.com/watch?v=B_-3F51QmKE">video</a> -->
                  <p>We propose an case-adaptive softmax representation to solve the distribution fixation problem in Deepfake Detection.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/eccv22-youshould.png' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2207.07889">
                    <papertitle>You Should Look at All Objects</papertitle>
                  </a>
                  <br>
                  Zhenchao Jin, Dongdong Yu, <strong>Luchuan Song</strong>,  Zehuan Yuan, Lequan Yu
                  <br>
                  <em>ECCV</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2207.07889">paper</a> /
                  <a href="https://github.com/CharlesPikachu/YSLAO">code</a>
                  <!-- <a href="https://www.youtube.com/watch?v=B_-3F51QmKE">video</a> -->
                  <p>We address the detection performance of large-scale objects are usually suppressed after introducing FPN.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/eccv22-ocr.png' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2207.11934">
                    <papertitle>Optimal Boxes: Boosting End-to-End Scene Text Recognition by Adjusting Annotated Bounding Boxes via Reinforcement Learning</papertitle>
                  </a>
                  <br>
                  Jingqun Tang, Wenming Qian, <strong>Luchuan Song</strong>, Xiena Dong, Lan Li, Xiang Bai
                  <br>
                  <em>ECCV</em>, 2022
                  <br>
                  <a href="https://arxiv.org/pdf/2207.11934">paper</a>
                  <!-- <a href="https://github.com/CharlesPikachu/YSLAO">code</a> -->
                  <!-- <a href="https://www.youtube.com/watch?v=B_-3F51QmKE">video</a> -->
                  <p>We attach reinforcement learning (DQN) to adjust the bbox in OCR.</p>
                </td>
              </tr>
  
              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/mm21.gif' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475196">
                    <papertitle>TACR-NET: Editing on Deep Video and Voice Portraits</papertitle>
                  </a>
                  <br>
                  <strong>Luchuan Song</strong>, Bin Liu, Guojun Yin, Xiaoyi Dong, Yufei Zhang, Jiaxuan Bai
                  <br>
                  <em>ACM'MM</em>, 2021
                  <br>
                  <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475196">paper</a> /
                  <!-- <a href="https://github.com/Songluchuan/Adaptive-Face-Forgery-Detection-in-Cross-Domain-ECCV-2022">code</a> -->
                  <a href="https://www.youtube.com/watch?v=HhgkuKgmmzs">video</a>
                  <p>We not only edit the appearance of the talking head, but also the voice. And we explored the relationship between the transfered voice features and lip-sync.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/cvpr21.png' style="width:270px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475196">
                    <papertitle>ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis</papertitle>
                  </a>
                  <br>
                  Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, <strong>Luchuan Song</strong>, Lu Sheng, Jing Shao, Ziwei Liu
                  <br>
                  <em>CVPR</em>, 2021 (<strong>oral</strong>)
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/He_ForgeryNet_A_Versatile_Benchmark_for_Comprehensive_Forgery_Analysis_CVPR_2021_paper.pdf">paper</a> /
                  <a href="https://yinanhe.github.io/projects/forgerynet.html">project page</a> /
                  <a href="https://github.com/yinanhe/forgerynet">code</a> /
                  <a href="https://youtu.be/e8XIL3Di2Y8">video</a> /
                  <a href="https://115.com/s/swnk84d3wl3?password=cvpr&#ForgeryNet">dataset</a> /
                  <a href="https://competitions.codalab.org/competitions/33386">challenge</a>
                  <p>ForgeryNet is by far the largest publicly available deep face forgery dataset in terms of data-scale (2.9M images, 221,247 videos), manipulations and annotations.</p>
                </td>
              </tr>


              

            </tbody>
          </table>




  </br>
  </br>
  </br>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <!-- <td style="padding:20px;width:100%;vertical-align:middle"> -->
                <heading>Experience</heading>
              </td>
            </tr>
          </tbody></table>
    
    
              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                  <tr>
                    <td style="padding:0px;width:25%;vertical-align:middle">
                      <img src='images/Adobe.png' style="width:150px;">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <strong>Adobe Research</strong>, Adobe, San Jose
                      <br>
                      <em>Research Scientist Intern</em>
                      <br>
                      May 2024 - Dec. 2024
                      <br>
                      Mentor(s): Yang Zhou, Zhan Xu, Yi Zhou and Deepali Aneja
                    </td>
                  </tr>


                  <tr>
                    <td style="padding:0px;width:25%;vertical-align:middle">
                      <img src='images/tencent.png' style="width:150px;">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <strong>Tencent YouTu Lab</strong>, Tencent, Shanghai
                      <br>
                      <em>Research Scientist Intern</em>
                      <br>
                      Jan. 2021 - May 2021
                      <br>
                      Mentor(s): Hao Liu
                    </td>
                  </tr>
                

                  <tr>
                    <td style="padding:0px;width:25%;vertical-align:middle">
                      <img src='images/sensetime.png' style="width:150px;">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <strong>SenseTime Research</strong>, SenseTime, Shanghai
                      <br>
                      <em>Research Scientist Intern</em>
                      <br>
                      Apr. 2020 - Dec. 2020
                      <br>
                      Mentor(s): Guojun Yin
                    </td>
                  </tr>


                  <tr>
                    <td style="padding:0px;width:25%;vertical-align:middle">
                      <img src='images/ruitian.png' style="width:150px;">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <strong>Ruitian Investment LLC</strong>, Shanghai
                      <br>
                      <em>Quantitative Researcher Intern & FTE Quantitative Trader</em>
                      <br>
                      Sep. 2018 - May 2019
                      <br>
                      Job: Explore alpha trading strategies on A-shares (actual $200 million), develop automated factor search system
                    </td>
                  </tr>


                </tbody>
              </table>


            </br>
            </br>
          </br>
          </br>
          </br>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      <tr>
                      <!-- <td style="padding:20px;width:100%;vertical-align:middle"> -->
                        <heading>Education</heading>
                      </td>
                    </tr>
                  </tbody></table>
            
            
                      <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
        
                          <tr>
                            <td style="padding:0px;width:25%;vertical-align:middle">
                              <img src='images/UR.png' style="width:100px;">
                            </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">
                              <strong>University of Rochester</strong>, New York
                              <br>
                              <em>Ph.D. in Computer Science</em>
                              <br>
                              Sep. 2022 - Present
                              <br>
                              Advisor: Chenliang Xu
                            </td>
                          </tr>
        
        
                          <tr>
                            <td style="padding:0px;width:25%;vertical-align:middle">
                              <img src='images/ustc.png' style="width:80px;">
                            </td>
                            <td style="padding:20px;width:75%;vertical-align:middle">
                              <strong>University of Science and Technology of China</strong>, Hefei
                              <br>
                              <em>B.Sc and M.Eng in EE</em>
                              <br>
                              Sep. 2014 - July 2021
                              <br>
                              Advisor: Nenghai Yu and Bin Liu
                            </td>
                          </tr>
                        
                        </tbody>
                      </table>

<!-- ##################################Teaching######################## -->
</br>
</br>
</br>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <heading>Teaching</heading>
  </td>
</tr>
</tbody></table>

  <table width="100%" align="center" border="0">
  <tr>
    <td width="100%" valign="center">
    <p>
      <a href="https://www.cs.rochester.edu/~cxu22/t/245F24/">
      <papertitle>CSC 245/445: Deep Learning - Fall 2024 & Fall 2023</papertitle> </a>
      <p>This course covers much of the recent advances in machine learning and artificial intelligence have been dominated by neural network approaches broadly described as deep learning.</p>

      <a href="https://www.cs.rochester.edu/~cxu22/t/249S24/">
      <papertitle>CSC 249/449: Machine Vision - Spring 2024</papertitle></a>
      <p>Fundamentals of computer vision, including image formation, elements of human vision, low-level image processing, and pattern recognition techniques.  </p>

      <a href="https://faculty.ustc.edu.cn/flowice/zh_CN/zdylm/679092/list/index.htm">
      <papertitle>Information Theory (B) - Fall 2018</papertitle></a>
      <p>This course mainly introduces the theories of source coding, channel coding and rate distortion. Other concepts, such as asymptotic equipartition property, entropy rate, and differential entropy, are also introduced.  </p>

  </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center">
                    <br>
                    Wonderful template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
